{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22ca87db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.cluster\n",
    "# from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from hmmlearn import hmm\n",
    "import numpy\n",
    "import os\n",
    "import scipy\n",
    "import glob\n",
    "from sklearn.metrics import classification_report\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa218cf",
   "metadata": {},
   "source": [
    "### Problem 1. Some clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f62a493",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA():\n",
    "    def __init__(self, n_components: int) -> None:\n",
    "        self.n_components = n_components\n",
    "        return\n",
    "\n",
    "    def _X_mean(self, X: numpy.ndarray) -> numpy.ndarray:\n",
    "        X_mean = numpy.mean(a=X, axis=1, keepdims=True)   # mean of matrix X  [X, ]\n",
    "        one_vec = numpy.ones(shape=(1, X.shape[1]))       # [1 x observations]\n",
    "        X_mean_mat = X_mean@one_vec                       # features mean matrix   [# of features x # of samples]\n",
    "        X_mean_center = X - X_mean_mat                    # mean-centered matrix X [# of features x # of samples]\n",
    "        return X_mean_center\n",
    "\n",
    "    def fit_transform(self, X: numpy.ndarray) -> tuple[numpy.ndarray, numpy.ndarray]:\n",
    "        X_mean_center = self._X_mean(X=X)\n",
    "        C = numpy.cov(m=X_mean_center, rowvar=True, bias=False)\n",
    "        eigenvals, eigenvecs = scipy.linalg.eig(a=C)\n",
    "        eigenval_indices = numpy.argsort(a=eigenvals)[::-1]\n",
    "        eigenvals = eigenvals[eigenval_indices].real\n",
    "        eigenvecs = eigenvecs[:, eigenval_indices].real\n",
    "        Lambda = scipy.linalg.inv(a=numpy.diag(v=numpy.sqrt(eigenvals[:self.n_components])))\n",
    "        U = eigenvecs[:, :self.n_components]\n",
    "        W = Lambda@U.T\n",
    "        Y = W@X_mean_center\n",
    "        return W, Y\n",
    "\n",
    "    def reconstruct(self, X: numpy.ndarray) -> numpy.ndarray:\n",
    "        X_mean_mat, X_mean_center = self._X_mean(X=X)\n",
    "        Z = self.U[:, :self.n_components].T@X_mean_center\n",
    "        X_hat = self.U[:, :self.n_components]@Z+X_mean_mat\n",
    "        return X_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9082a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans():\n",
    "    def __init__(self, n_clusters: int, tol: float) -> None:\n",
    "        self.n_clusters = n_clusters\n",
    "        self.tol = tol\n",
    "        self.cluster_centers_ = None\n",
    "        self.labels_ = None\n",
    "\n",
    "        return\n",
    "\n",
    "    def fit(self, X: numpy.ndarray) -> tuple[numpy.ndarray, numpy.ndarray]:\n",
    "        cluster_center_indices = numpy.random.choice(a=X.shape[0], size=self.n_clusters, replace=False)\n",
    "        cluster_centers = X[cluster_center_indices]\n",
    "    \n",
    "        while True:\n",
    "            dists = scipy.linalg.norm(a=(X[:, numpy.newaxis]-cluster_centers), axis=2)\n",
    "            labels = numpy.argmin(a=dists, axis=1)\n",
    "\n",
    "            new_cluster_centers = []\n",
    "            for i in range(self.n_clusters):\n",
    "                cluster_mu = X[labels==i].mean(axis=0)\n",
    "                new_cluster_centers.append(cluster_mu)\n",
    "            new_cluster_centers = numpy.vstack(tup=new_cluster_centers, dtype=numpy.float32)\n",
    "\n",
    "            if numpy.all(scipy.linalg.norm(a=(new_cluster_centers-cluster_centers), axis=0) <= self.tol):\n",
    "                break\n",
    "\n",
    "            cluster_centers = new_cluster_centers\n",
    "\n",
    "        self.cluster_centers_ = cluster_centers\n",
    "        self.labels_ = labels\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X: numpy.ndarray) -> numpy.ndarray:\n",
    "        dists = scipy.linalg.norm(a=(X[:, numpy.newaxis]-self.cluster_centers_), axis=2)\n",
    "\n",
    "        return numpy.argmin(a=dists, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70231f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_frames(times: float, sr: int, hop_length: int, n_fft: int=None) -> int:\n",
    "    return librosa.time_to_frames(times=times, sr=sr, hop_length=hop_length, n_fft=n_fft)\n",
    "\n",
    "def create_labels(label_filepath: str, sr: int, hop_length: int, n_fft: int=None) -> numpy.ndarray:\n",
    "    dtypes = [(\"start_time\", \"float64\"), (\"end_time\", \"float64\"), (\"label\", \"U10\")]\n",
    "    label_data = np.genfromtxt(fname=label_filepath, dtype=dtypes, delimiter='\\t', skip_header=0, skip_footer=0)\n",
    "\n",
    "    classes = numpy.array([label_info[2] for label_info in label_data])\n",
    "    classes = numpy.unique(ar=classes)\n",
    "\n",
    "    frame_labels = numpy.array(object=[], dtype=numpy.uint8)\n",
    "    for label_info in label_data:\n",
    "        start_time, end_time, label = label_info\n",
    "        start_frame = time_to_frames(times=start_time, sr=sr, hop_length=hop_length, n_fft=N_FFT)\n",
    "        end_frame = time_to_frames(times=end_time, sr=sr, hop_length=hop_length, n_fft=N_FFT)\n",
    "        label_id = numpy.where(classes==label)[0]\n",
    "        labels = numpy.full(shape=(end_frame-start_frame), fill_value=label_id, dtype=numpy.uint8)\n",
    "        frame_labels = numpy.append(arr=frame_labels, values=labels)\n",
    "    frame_labels = numpy.append(arr=frame_labels, values=label_id)\n",
    "    print(frame_labels.shape)\n",
    "    \n",
    "    return frame_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c66c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_RATE = 22050\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = N_FFT//4\n",
    "WIN_LENGTH = N_FFT//2\n",
    "\n",
    "dataset_dir = \"friends\"\n",
    "\n",
    "audio_filepath = dataset_dir + \"/friends.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d56b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, _ = librosa.load(path=audio_filepath, sr=SAMPLING_RATE, dtype=numpy.float64)\n",
    "spec = librosa.stft(y=y, n_fft=N_FFT, hop_length=HOP_LENGTH, win_length=WIN_LENGTH)\n",
    "spec = numpy.abs(spec)\n",
    "spec_sqrt = numpy.sqrt(spec)\n",
    "\n",
    "matplotlib.pyplot.rc('font', family='serif')\n",
    "matplotlib.pyplot.rc(group=\"text\", usetex=True)\n",
    "fig, ax = matplotlib.pyplot.subplots(1, 2, figsize=(15, 5))\n",
    "img_0 = librosa.display.specshow(data=spec_sqrt, sr=SAMPLING_RATE, hop_length=HOP_LENGTH, n_fft=N_FFT,\n",
    "                                 win_length=WIN_LENGTH, x_axis='s', y_axis='log', ax=ax[0])\n",
    "fig.colorbar(img_0, ax=[ax[0]], label=r\"$\\sqrt{\\textnormal{Magnitude}}$\")\n",
    "ax[0].set_title(\"Spectrogram\")\n",
    "\n",
    "spec_db = librosa.amplitude_to_db(S=spec, ref=numpy.max)\n",
    "img_1 = librosa.display.specshow(data=spec_db, sr=SAMPLING_RATE, hop_length=HOP_LENGTH, n_fft=N_FFT,\n",
    "                                 win_length=WIN_LENGTH, x_axis='s', y_axis=\"log\", ax=ax[1])\n",
    "fig.colorbar(img_1, ax=[ax[1]], format=r\"$%+2.0f$ dB\")\n",
    "ax[1].set_title(\"Spectrogram\")\n",
    "\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31165690",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spec.shape)\n",
    "pca = PCA(n_components=40)\n",
    "W, Y = pca.fit_transform(X=spec)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25e24b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = librosa.feature.mfcc(y=y, sr=SAMPLING_RATE, n_mfcc=20, n_fft=N_FFT, hop_length=HOP_LENGTH,\n",
    "                             win_length=WIN_LENGTH)\n",
    "print(mfccs.shape)\n",
    "pca1 = PCA(n_components=10)\n",
    "W1, Y1 = pca1.fit_transform(X=mfccs)\n",
    "\n",
    "feat = numpy.vstack((Y, Y1), dtype=numpy.float64)\n",
    "print(feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d72476",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, tol=1e-5).fit(X=spec.T)\n",
    "labels = kmeans.predict(X=spec.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bfed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels.shape)\n",
    "print(labels[7000:8000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c913ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# times = librosa.frames_to_time(frames=numpy.arange(labels.shape[0]), sr=SAMPLING_RATE, hop_length=HOP_LENGTH, n_fft=N_FFT)\n",
    "# print(times.shape)\n",
    "# print(times[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f6cc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_filepath = dataset_dir + \"/labels.txt\"\n",
    "\n",
    "frame_labels = create_labels(label_filepath=label_filepath, sr=SAMPLING_RATE, hop_length=HOP_LENGTH, n_fft=N_FFT)\n",
    "print(frame_labels[7000:8000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aff9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_times = librosa.frames_to_time(frames=np.arange(spec_db.shape[1]), sr=sr)\n",
    "print(frame_times)\n",
    "print(len(frame_times))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19532a2",
   "metadata": {},
   "source": [
    "### Problem 2. Recognition of temporal sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2119bf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_RATE = 22050\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = N_FFT//4\n",
    "WIN_LENGTH = N_FFT//2\n",
    "N_MFCC = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103af101",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_map = {\"zero\": 0, \"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4,\n",
    "           \"five\": 5, \"six\": 6, \"seven\": 7, \"eight\": 8, \"nine\": 9}\n",
    "\n",
    "dataset_dir = \"digits\"\n",
    "digit_dirs = os.listdir(path=dataset_dir)\n",
    "\n",
    "fig, axs = plt.subplots(10, 5, figsize=(10, 20), gridspec_kw={\"width_ratios\": [1, 1, 1, 1, 1.3]},\n",
    "                        sharex=True, sharey=True)\n",
    "matplotlib.pyplot.subplots_adjust(left=0.05, right=0.95, bottom=0.05, top=0.95, wspace=0.50, hspace=0.50)\n",
    "\n",
    "for i, digit_dir in enumerate(digit_dirs):\n",
    "    for j in range(5):\n",
    "        audio_filepath = f\"digits/{digit_dir}/{digit_dir}_{j}.wav\"\n",
    "        y, _ = librosa.load(path=\"digits/one/one_0.wav\", sr=SAMPLING_RATE, dtype=numpy.float64)\n",
    "        spec = librosa.stft(y=y, n_fft=N_FFT, hop_length=HOP_LENGTH, win_length=WIN_LENGTH)\n",
    "        spec = numpy.abs(spec)\n",
    "        spec_sqrt = numpy.sqrt(spec)\n",
    "        img = librosa.display.specshow(data=spec_sqrt, sr=SAMPLING_RATE, hop_length=HOP_LENGTH, n_fft=N_FFT,\n",
    "                                       win_length=WIN_LENGTH, x_axis='s', y_axis=\"log\", ax=axs[i, j])\n",
    "        axs[i, j].set_title(f'Digit {num_map[digit_dir]}')\n",
    "        axs[i, j].set_xlabel(r'Time $(s)$')\n",
    "        axs[i, j].set_ylabel(r'Frequency (Hz)')\n",
    "\n",
    "        if j == 4:\n",
    "            axs[i, j].set_aspect('auto')\n",
    "            cbar = fig.colorbar(img, ax=axs[i, j], pad=0.1)\n",
    "            cbar.set_label(label=r\"$\\sqrt{\\textnormal{Magnitude}}$\")\n",
    "\n",
    "matplotlib.pyplot.tight_layout()\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a263f2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpokenDigit():\n",
    "    def __init__(self, dataset_dir: str, num_map: dict) -> None:\n",
    "        self.classes = numpy.array(object=list(num_map.values()), dtype=numpy.uint8)\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.__load_data_labels(dataset_dir=dataset_dir, num_map=num_map)\n",
    "        self.data_train_ = []\n",
    "        self.labels_train_ = []\n",
    "        self.data_test_ = []\n",
    "        self.labels_test_ = []\n",
    "\n",
    "        return\n",
    "\n",
    "    def __load_data_labels(self, dataset_dir: str, num_map: dict) -> None:\n",
    "        digit_dirs = list(num_map.keys())\n",
    "\n",
    "        for i, digit_dir in enumerate(digit_dirs):\n",
    "            filepath = os.path.join(dataset_dir, digit_dir, f\"{digit_dir}_*.wav\")\n",
    "            filepaths = glob.glob(pathname=filepath)\n",
    "\n",
    "            data = []\n",
    "            for filepath in filepaths:\n",
    "                y, sr = librosa.load(path=filepath)\n",
    "                m = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC, n_fft=N_FFT, hop_length=HOP_LENGTH, win_length=WIN_LENGTH,\n",
    "                                         dtype=numpy.float64)\n",
    "                data.append(m.T)\n",
    "\n",
    "            self.data.append(data)\n",
    "            labels = numpy.full(shape=(len(filepaths)), fill_value=i, dtype=numpy.uint8)\n",
    "            \n",
    "            self.labels.append(labels)\n",
    "\n",
    "        return\n",
    "\n",
    "    def train_test_split(\n",
    "            self,\n",
    "            test_pct: float,\n",
    "            train_pct: float,\n",
    "            rand_state: int\n",
    "    ) -> tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray]:\n",
    "        if rand_state is not None:\n",
    "            numpy.random.seed(seed=rand_state)\n",
    "\n",
    "        for data, label in zip(self.data, self.labels):\n",
    "            test_size = int(len(data)*test_pct)\n",
    "            indices = numpy.arange(len(data))\n",
    "            numpy.random.shuffle(indices)\n",
    "            indices_test = indices[:test_size]\n",
    "\n",
    "            data_class_train = []\n",
    "            labels_class_train = []\n",
    "            for i, (d, l) in enumerate(zip(data, label)):\n",
    "                if i not in indices_test:  \n",
    "                    data_class_train.append(d)\n",
    "                    labels_class_train.append(l)\n",
    "                else:\n",
    "                    self.data_test_.append(d)\n",
    "                    self.labels_test_.append(l)\n",
    "\n",
    "            self.data_train_.append(data_class_train)\n",
    "            self.labels_train_.append(labels_class_train)\n",
    "\n",
    "        self.labels_test_ = numpy.hstack(tup=self.labels_test_, dtype=numpy.uint8)\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebbddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpokenDigitRecog():\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset_dir: str,\n",
    "            num_map: dict,\n",
    "            test_pct: float,\n",
    "            train_pct: float,\n",
    "            rand_state: int\n",
    "    ) -> None:\n",
    "        self.num_map = num_map\n",
    "        self.spoken_digit = SpokenDigit(dataset_dir=dataset_dir, num_map=num_map)\n",
    "        self.spoken_digit.train_test_split(test_pct=test_pct, train_pct=train_pct, rand_state=rand_state)\n",
    "        self.models_ = {}\n",
    "\n",
    "    def train_HMM(self) -> None:\n",
    "        for i in range(len(self.spoken_digit.data_train_)):\n",
    "            model = hmm.GMMHMM(n_components=10)\n",
    "            data_train = self.spoken_digit.data_train_[i]\n",
    "            data_train = numpy.vstack(tup=data_train, dtype=numpy.float64)\n",
    "            model.fit(data_train)\n",
    "            self.models_[i] = model\n",
    "\n",
    "        return\n",
    "\n",
    "    def test_HMM(self) -> None:\n",
    "        preds = numpy.array(object=[], dtype=numpy.uint8)\n",
    "\n",
    "        for i in range(len(self.spoken_digit.data_test_)):\n",
    "            data_test = self.spoken_digit.data_test_[i]\n",
    "            scores = numpy.array(object=[], dtype=numpy.float64)\n",
    "            for cls in list(self.models_.keys()):\n",
    "                model = self.models_[cls]\n",
    "                score = model.score(data_test)\n",
    "                scores = numpy.append(arr=scores, values=score)\n",
    "            pred = numpy.argmax(a=scores, axis=0)\n",
    "            preds = numpy.append(arr=preds, values=pred)\n",
    "\n",
    "        report = classification_report(y_true=self.spoken_digit.labels_test_, y_pred=preds,\n",
    "                                       labels=list(self.num_map.values()),\n",
    "                                       target_names=list(self.num_map.keys()))\n",
    "        print(report)\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c34c6c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sdr = SpokenDigitRecog(dataset_dir=dataset_dir, num_map=num_map,\n",
    "                       test_pct=0.25, train_pct=0.75, rand_state=None)\n",
    "sdr.train_HMM()\n",
    "sdr.test_HMM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a985cf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means clustering\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters=3, tol=1e-3)\n",
    "kmeans_labels = kmeans.fit_predict(spec.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c682267",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(kmeans_labels))\n",
    "print(kmeans_labels[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2453ca87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMM clustering\n",
    "gmm = GaussianMixture(n_components=3)\n",
    "gmm_labels = gmm.fit_predict(spec.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d94660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(gmm_labels))\n",
    "print(gmm_labels[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1cc4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HMM clustering\n",
    "hmm_model = hmm.GMMHMM(n_components=3)\n",
    "hmm_model.fit(spec.T)\n",
    "hmm_labels = hmm_model.predict(spec.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9d3eb1",
   "metadata": {},
   "source": [
    "### Problem 3. Activity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8097f417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class_map(class_filepath: str, class_names: list[str]) -> dict:\n",
    "    dtypes = [(\"labels\", \"uint8\"), (\"activity\", \"U20\")]\n",
    "    class_info = numpy.genfromtxt(fname=class_filepath, dtype=dtypes, delimiter=' ', skip_header=0, skip_footer=0)\n",
    "    class_map = dict(class_info)\n",
    "    class_map = {k-1: v for k, v in class_map.items() if v in class_names}\n",
    "\n",
    "    return class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "92ae4957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HAR():\n",
    "    def __init__(self, dataset_dir: str, classes: numpy.ndarray) -> None:\n",
    "        self.classes = classes\n",
    "        self.data = None\n",
    "        self.labels = None\n",
    "        self.__load_data(dataset_dir=dataset_dir)\n",
    "        self.__load_labels(dataset_dir=dataset_dir)\n",
    "        self.__filter(classes=classes)\n",
    "        self.data_train_ = []\n",
    "        self.labels_train_ = []\n",
    "        self.data_test_ = None\n",
    "        self.labels_test_ = None\n",
    "        self.data_train_test_ = None\n",
    "        self.labels_train_test_ = None\n",
    "\n",
    "        return\n",
    "\n",
    "    def __load_data(self, dataset_dir: str) -> None:\n",
    "        x_filepath = os.path.join(dataset_dir, \"test\", \"Inertial_Signals\", \"total_acc_x_test.txt\")\n",
    "        y_filepath = os.path.join(dataset_dir, \"test\", \"Inertial_Signals\", \"total_acc_y_test.txt\")\n",
    "        z_filepath = os.path.join(dataset_dir, \"test\", \"Inertial_Signals\", \"total_acc_z_test.txt\")\n",
    "        x = numpy.loadtxt(fname=x_filepath, dtype=numpy.float64)\n",
    "        y = numpy.loadtxt(fname=y_filepath, dtype=numpy.float64)\n",
    "        z = numpy.loadtxt(fname=z_filepath, dtype=numpy.float64)\n",
    "        self.data = numpy.stack(arrays=(x, y, z), axis=2)\n",
    "\n",
    "        return\n",
    "\n",
    "    def __load_labels(self, dataset_dir: str) -> None:\n",
    "        label_filepath = os.path.join(dataset_dir, \"test\", \"y_test.txt\")\n",
    "        self.labels = numpy.loadtxt(fname=label_filepath, dtype=numpy.uint8)\n",
    "        self.labels -= 1\n",
    "\n",
    "        return\n",
    "\n",
    "    def __filter(self, classes: list[int]) -> None:\n",
    "        mask = numpy.isin(element=self.labels, test_elements=classes)\n",
    "        N = numpy.count_nonzero(a=mask)\n",
    "        self.data = self.data[mask]\n",
    "        self.labels = self.labels[mask]\n",
    "\n",
    "        assert N == self.data.shape[0] == self.labels.shape[0]\n",
    "\n",
    "    def train_test_split(\n",
    "            self,\n",
    "            test_pct: float,\n",
    "            train_pct: float,\n",
    "            rand_state: int\n",
    "    ) -> tuple[numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray]:\n",
    "        if rand_state is not None:\n",
    "            numpy.random.seed(seed=rand_state)\n",
    "\n",
    "#         mu = numpy.mean(a=self.data, axis=0, dtype=numpy.float64)\n",
    "#         self.data -= mu\n",
    "\n",
    "#         data_train = []\n",
    "#         labels_train = []\n",
    "        data_test = []\n",
    "        labels_test = []\n",
    "        for cls in self.classes:\n",
    "            indices_cls = numpy.where(self.labels==cls)[0]\n",
    "            test_size = int(indices_cls.shape[0]*test_pct)\n",
    "            indices_cls_test = numpy.random.choice(a=indices_cls, size=test_size, replace=False)\n",
    "            # print(indices_cls_test.shape)\n",
    "            mask = numpy.isin(element=indices_cls, test_elements=indices_cls_test, assume_unique=True)\n",
    "            indices_cls_train = indices_cls[~mask]\n",
    "            # print(indices_cls_train.shape)\n",
    "            self.data_train_.append(self.data[indices_cls_train])\n",
    "            self.labels_train_.append(self.labels[indices_cls_train])\n",
    "            data_test.append(self.data[indices_cls_test])\n",
    "            labels_test.append(self.labels[indices_cls_test])\n",
    "\n",
    "        self.data_train_test_ = numpy.vstack(tup=self.data_train_, dtype=numpy.float64)\n",
    "        self.labels_train_test_ = numpy.hstack(tup=self.labels_train_, dtype=numpy.uint8)\n",
    "        # print(self.data_train_test_.shape)\n",
    "        # print(self.labels_train_test_.shape)\n",
    "        \n",
    "        assert self.data_train_test_.shape[0] == self.labels_train_test_.shape[0]\n",
    "\n",
    "        self.data_test_ = numpy.vstack(tup=data_test, dtype=numpy.float64)\n",
    "        self.labels_test_ = numpy.hstack(tup=labels_test, dtype=numpy.uint8)\n",
    "        # print(self.data_test_.shape)\n",
    "        # print(self.labels_test_.shape)\n",
    "        assert self.data_test_.shape[0] == self.labels_test_.shape[0]\n",
    "\n",
    "#         for a,b in zip(data_train, labels_train):\n",
    "#             print(len(a), len(b))\n",
    "#         print(\"-------\")\n",
    "#         for a,b in zip(data_test, labels_test):\n",
    "#             print(a.shape, b.shape)\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "12295549",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAR():\n",
    "    def __init__(self, p: int) -> None:\n",
    "        self.p = p\n",
    "        self.model = LinearRegression()\n",
    "\n",
    "        return\n",
    "\n",
    "    def __train_x(self, X: numpy.ndarray, p: int, steps: int) -> numpy.ndarray:\n",
    "        n = X.shape[0]\n",
    "\n",
    "        data_train = []\n",
    "        for i in range(steps):\n",
    "            data_train.append(X[i:i+p].flatten(order='C'))\n",
    "\n",
    "        X = numpy.vstack(tup=data_train, dtype=numpy.float64)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def fit(self, X: numpy.ndarray, y: numpy.ndarray, p:int, steps: int) -> None:\n",
    "        X = self.__train_x(X=X, p=p, steps=steps)\n",
    "        self.model.fit(X=X, y=y)\n",
    "\n",
    "        return\n",
    "\n",
    "    def predict(self, X: numpy.ndarray, steps: int) -> None:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ab41775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActRecog():\n",
    "    def __init__(self, dataset_dir: str, class_map: dict, lag_order: int) -> None:\n",
    "        self.classes = numpy.unique(ar=list(class_map.keys()))\n",
    "        self.class_names = list(class_map.values())\n",
    "        self.lag_order = lag_order\n",
    "        self.har = HAR(dataset_dir=dataset_dir, classes=self.classes)\n",
    "        self.har.train_test_split(test_pct=0.5, train_pct=0.5, rand_state=None)\n",
    "        self.VAR_models_ = {}\n",
    "\n",
    "        return\n",
    "\n",
    "    def __test_x(self, X: numpy.ndarray, p: int, steps: int) -> numpy.ndarray:\n",
    "        n = X.shape[0]\n",
    "\n",
    "        data_test = []\n",
    "        for i in range(steps):\n",
    "            data_test.append(X[i:i+p].flatten(order='C'))\n",
    "\n",
    "        X = numpy.vstack(tup=data_test, dtype=numpy.float64)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def train_VAR_models(self, p: int, steps: int) -> None:\n",
    "        for cls in self.classes:\n",
    "            W = []\n",
    "            b = []\n",
    "\n",
    "            for X, y in zip(self.har.data_train_[cls], self.har.labels_train_[cls]):\n",
    "                y = X[p:p+steps]\n",
    "                var = VAR(p=p)\n",
    "                var.fit(X=X, y=y, p=p, steps=steps)\n",
    "#                 print(var.model.coef_)\n",
    "#                 print(var.model.intercept_)\n",
    "                W.append(var.model.coef_)\n",
    "                b.append(var.model.intercept_)\n",
    "\n",
    "            W = numpy.stack(arrays=W, axis=0, dtype=numpy.float64)\n",
    "            W = numpy.mean(a=W, axis=0, dtype=numpy.float64)\n",
    "            b = numpy.stack(arrays=b, axis=0, dtype=numpy.float64)\n",
    "            b = numpy.mean(a=b, axis=0, dtype=numpy.float64)\n",
    "#             print(W.shape)\n",
    "#             print(b.shape)\n",
    "            self.VAR_models_[cls] = [W, b]\n",
    "\n",
    "#                 model = VAR(X)\n",
    "#                 var_res = model.fit(maxlags=self.lag_order, method=\"ols\", ic=None, trend='c')\n",
    "#                 VAR_models_class.append(var_res.params)\n",
    "\n",
    "#             VAR_models_class = numpy.stack(arrays=VAR_models_class, axis=0, dtype=numpy.float64)\n",
    "#             self.VAR_models_[i] = VAR_models_class\n",
    "\n",
    "    def test_VAR_models(self, p: int, steps: int, data: str) -> None:\n",
    "        if data == \"train\":\n",
    "            data_test = self.har.data_train_test_\n",
    "            labels_test = self.har.labels_train_test_\n",
    "        elif data == \"test\":\n",
    "            data_test = self.har.data_test_\n",
    "            labels_test = self.har.labels_test_\n",
    "        else:\n",
    "            print(\"[ERROR]: Invalid data type.\")\n",
    "\n",
    "        preds = numpy.array(object=[], dtype=numpy.uint8)\n",
    "        for X, y in zip(data_test, labels_test):\n",
    "            loss = numpy.array(object=[], dtype=numpy.float64)\n",
    "            for cls in range(0, self.classes.size):\n",
    "                forecasts = []\n",
    "                W, b = self.VAR_models_[cls]\n",
    "                x = X[:p].flatten(order='F')\n",
    "                for step in range(steps):\n",
    "                    forecast = x@W.T + b\n",
    "                    forecasts.append(forecast)\n",
    "                    x = numpy.hstack(tup=[x[3:], forecast], dtype=numpy.float64)\n",
    "\n",
    "                forecasts = numpy.vstack(tup=forecasts, dtype=numpy.float64)\n",
    "                y_true = X[self.lag_order:self.lag_order+steps]\n",
    "                mse = scipy.linalg.norm(a=(forecasts-y_true))\n",
    "                loss = numpy.append(arr=loss, values=mse)\n",
    "            pred = numpy.argmin(a=loss, axis=0)\n",
    "            preds = numpy.append(arr=preds, values=pred)\n",
    "        print(preds)\n",
    "        print(self.har.labels_test_)\n",
    "        report = classification_report(y_true=labels_test, y_pred=preds, labels=self.classes,\n",
    "                                       target_names=self.class_names)\n",
    "        print(report)\n",
    "\n",
    "        conf_mat_test = sklearn.metrics.confusion_matrix(y_true=labels_test, y_pred=preds,\n",
    "                                                         labels=self.classes)\n",
    "        print(conf_mat_test)\n",
    "\n",
    "#         preds = numpy.array(object=[], dtype=numpy.uint8)\n",
    "#         for X, y in zip(data_test, labels_test):\n",
    "#             loss = numpy.array(object=[], dtype=numpy.float64)\n",
    "#             for cls in range(0, self.classes.size):\n",
    "#                 forecasts = []\n",
    "#                 for step in range(steps):\n",
    "#                     W = numpy.mean(a=self.VAR_models_[cls], axis=0)\n",
    "#                     X_test = X[-self.lag_order:].flatten(order='F')\n",
    "#                     X_test = X_test[:, numpy.newaxis]\n",
    "#                     X_test = numpy.append(X_test, 1)\n",
    "#                     forecast = X_test@W\n",
    "#                     X_test = numpy.reshape(a=X_test[:-1], newshape=(-1, 3), order='F')\n",
    "#                     X_test = numpy.vstack((X_test[1:], forecast))\n",
    "#                     forecasts.append(forecast)\n",
    "#                 forecasts = numpy.vstack(tup=forecasts, dtype=numpy.float64)\n",
    "#                 y_true = X[self.lag_order:self.lag_order+steps]\n",
    "#                 mse = scipy.linalg.norm(a=(forecasts-y_true))\n",
    "#                 loss = numpy.append(arr=loss, values=mse)\n",
    "#             pred = numpy.argmin(a=loss, axis=0)\n",
    "#             preds = numpy.append(arr=preds, values=pred)\n",
    "#         print(preds)\n",
    "#         print(self.har.labels_test_)\n",
    "#         report = classification_report(y_true=labels_test, y_pred=preds, labels=self.classes,\n",
    "#                                        target_names=self.class_names)\n",
    "#         print(report)\n",
    "\n",
    "#         conf_mat_test = sklearn.metrics.confusion_matrix(y_true=labels_test, y_pred=preds,\n",
    "#                                                          labels=self.classes)\n",
    "#         print(conf_mat_test)\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e4214fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAG_ORDER = 50\n",
    "STEPS = 50\n",
    "\n",
    "dataset_dir = \"HAR\"\n",
    "class_filepath = dataset_dir + \"/activity_labels.txt\"\n",
    "class_names = [\"WALKING\", \"WALKING_UPSTAIRS\", \"WALKING_DOWNSTAIRS\", \"SITTING\"]\n",
    "class_map = create_class_map(class_filepath=class_filepath, class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c6c29a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 0 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 2 2 3 2 2 2 2 2 2 2 0\n",
      " 3 2 2 0 2 2 2 2 2 2 2 2 2 0 2 2 2 0 0 2 0 2 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 0 0 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 2 0 0 2 0 2 0 0 0 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 2 0 2 2 2 2\n",
      " 2 0 0 0 2 0 0 0 2 2 2 2 0 0 0 2 1 2 2 2 2 0 2 1 2 2 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 2 2 2 2 2 2 0 0 0 0 0 2 0 0 3 2 2 2 0 2 1\n",
      " 0 0 0 1 1 0 0 2 1 1 2 2 2 2 1 1 1 2 1 0 2 2 2 2 0 1 1 0 0 3 3 3 3 3 3 3 3\n",
      " 3 0 0 0 2 0 0 2 2 2 2 0 0 0 0 0 0 0 1 0 0 0 0 2 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 2 2 2 0 0 0 0 0 1 0 0 0 0 0 2 2 1 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 2 2 0 2 0 0 3 3 3 3 3 3 3 3 3 2\n",
      " 3 3 3 3 3 3 2 3 3 3 2 3 3 3 3 3 3 3 2 2 0 3 2 2 2 2 3 0 2 2 2 2 2 2 0 2 0\n",
      " 2 0 0 1 0 0 2 0 0 2 2 0 3 2 3 3 3 3 3 3 3 2 0 1 2 1 0 0 0 1 0 0 2 0 0 2 0\n",
      " 2 0 0 2 2 0 1 1 1 0 2 1 1 1 1 1 0 2 0 2 0 2 0 0 0 2 2 2 0 0 2 2 2 2 0 2 2\n",
      " 2 2 2 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 2 2 2 0 0 2 2 2 0 0 2 0\n",
      " 2 2 2 0 0 2 2 2 2 2 2 2 0 0 0 0 0 1 0 1 2 2 2 2 2 0 2 2 3 3 3 3 3 3 3 3 2\n",
      " 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 3 2 3 3 3 3 3 2 2 2 2 2 2 2 2 3 3 3 3 3\n",
      " 3 3 3 3 3 0 0 0 0 0 0 0 0 0 0 2 2 2 2 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 2 2\n",
      " 2 2 2 2 2 2 2 2 3 3 3 3 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "           WALKING       0.40      0.33      0.36       248\n",
      "  WALKING_UPSTAIRS       0.59      0.60      0.59       236\n",
      "WALKING_DOWNSTAIRS       0.33      0.34      0.34       210\n",
      "           SITTING       0.68      0.77      0.72       246\n",
      "\n",
      "          accuracy                           0.51       940\n",
      "         macro avg       0.50      0.51      0.50       940\n",
      "      weighted avg       0.50      0.51      0.51       940\n",
      "\n",
      "[[ 82  58  63  45]\n",
      " [ 53 141  32  10]\n",
      " [ 62  42  71  35]\n",
      " [ 10   0  47 189]]\n"
     ]
    }
   ],
   "source": [
    "actrecog = ActRecog(dataset_dir=dataset_dir, class_map=class_map, lag_order=LAG_ORDER)\n",
    "actrecog.train_VAR_models(p=100, steps=28)\n",
    "actrecog.test_VAR_models(p=100, steps=28, data=\"train\")\n",
    "# actrecog.test_VAR_models(steps=STEPS, data=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac040d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8bc96e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
